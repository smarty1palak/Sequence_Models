{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= open('dinos.txt','r').read()\n",
    "data= data.lower()\n",
    "unique_chars= list(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_chars, total_unique_chars= len(data), len(unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_char= {}\n",
    "char_index= {}\n",
    "for i,v in enumerate(unique_chars):\n",
    "    index_char[i]=v\n",
    "    char_index[v]=i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=[]\n",
    "fo = open(\"dinos.txt\", \"r\")\n",
    "train = fo.readlines()\n",
    "for i in range(0,len(train)):\n",
    "    train[i]= train[i].lower().strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=2000\n",
    "a_dim=100\n",
    "x_dim=total_unique_chars\n",
    "y_dim=total_unique_chars\n",
    "\n",
    "Wax=np.random.uniform(-np.sqrt(1./a_dim), np.sqrt(1./a_dim), (a_dim, x_dim))\n",
    "Wya=np.random.uniform(-np.sqrt(1./a_dim), np.sqrt(1./a_dim), (y_dim, a_dim))\n",
    "Waa=np.random.uniform(-np.sqrt(1./a_dim), np.sqrt(1./a_dim), (a_dim, a_dim))\n",
    "b= np.zeros((a_dim,1))\n",
    "by= np.zeros((y_dim,1))\n",
    "parameters={\"Wax\":Wax, \"Wya\":Wya, \"Waa\":Waa, \"b\":b, \"by\":by}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix, seed):\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    x = np.zeros((len(char_to_ix),1))\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    indices = []\n",
    "    idx = -1 \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "        a = np.tanh(Wax.dot(x) + Waa.dot(a_prev) + b)\n",
    "        z = Wya.dot(a) + by\n",
    "        y = softmax(z)\n",
    "        np.random.seed(counter+seed) \n",
    "        idx = np.random.choice(list(range(vocab_size)), p=y.ravel())\n",
    "        indices.append(idx)\n",
    "        x = np.zeros_like(x)\n",
    "        x[idx] = 1\n",
    "        a_prev = a\n",
    "        seed += 1\n",
    "        counter +=1\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_forward(parameters, a_prev, x):\n",
    "    \n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b) \n",
    "    p_t = softmax(np.dot(Wya, a_next) + by) \n",
    "    return a_next, p_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(X, Y, a0, parameters):\n",
    "    \n",
    "    x, a, y_hat = {}, {}, {}\n",
    "    a[-1] = np.copy(a0)\n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(0,len(X)):\n",
    "        x[t] = np.zeros((total_unique_chars,1)) \n",
    "        if (X[t] != None):\n",
    "            x[t][X[t]] = 1\n",
    "        a[t], y_hat[t] = rnn_step_forward(parameters, a[t-1], x[t])\n",
    "        loss -= np.log(y_hat[t][Y[t],0])\n",
    "        \n",
    "    cache = (y_hat, a, x)\n",
    "        \n",
    "    return loss, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    \n",
    "    gradients['dWya'] += np.dot(dy, a.T)\n",
    "    gradients['dby'] += dy\n",
    "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop into h\n",
    "    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n",
    "    gradients['db'] += daraw\n",
    "    gradients['dWax'] += np.dot(daraw, x.T)\n",
    "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
    "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    gradients = {}\n",
    "    \n",
    "    (y_hat, a, x) = cache\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
    "    gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n",
    "    gradients['da_next'] = np.zeros_like(a[0])\n",
    "    \n",
    "    for t in reversed(range(len(X))):\n",
    "        dy = np.copy(y_hat[t])\n",
    "        dy[Y[t]] -= 1\n",
    "        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t-1])\n",
    "    \n",
    "    return gradients, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        gradient[:] = np.clip(gradient, -maxValue, maxValue)\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, lr):\n",
    "\n",
    "    parameters['Wax'] += -lr * gradients['dWax']\n",
    "    parameters['Waa'] += -lr * gradients['dWaa']\n",
    "    parameters['Wya'] += -lr * gradients['dWya']\n",
    "    parameters['b']  += -lr * gradients['db']\n",
    "    parameters['by']  += -lr * gradients['dby']\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X,Y, a_prev, parameters, lr=0.01):\n",
    "    loss, cache = rnn_forward(X,Y,a_prev,parameters)\n",
    "    gradient ,a = rnn_backward(X,Y,parameters, cache)\n",
    "    gradient    = clip(gradient,5)\n",
    "    parameters  = update_parameters(parameters, gradient, lr)\n",
    "        \n",
    "    return loss, gradient, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_ix):\n",
    "    txt = ''.join(index_char[ix] for ix in sample_ix)\n",
    "    txt = txt[0].upper() + txt[1:] \n",
    "    print ('%s' % (txt, ), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    fo = open(\"dinos.txt\", \"r\")\n",
    "    train = fo.readlines()\n",
    "    for i in range(0,len(train)):\n",
    "        train[i]= train[i].lower().strip('\\n')\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(train)\n",
    "    a_prev = np.zeros((a_dim, 1))\n",
    "    for i in range(0,epochs):\n",
    "        for j in range(0,len(train)):\n",
    "            X = [None] + [char_index[ch] for ch in train[j]]\n",
    "            Y = X[1:] + [char_index[\"\\n\"]]\n",
    "            curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)\n",
    "            \n",
    "        if(i%10==0):\n",
    "            print('Epoch:%d Iteration:%d--->Loss:%f'%(i,j,curr_loss)+'\\n')\n",
    "            seed=0\n",
    "            for name in range(0,5):\n",
    "                sampled_indices = sample(parameters, char_index, seed)\n",
    "                print_sample(sampled_indices)\n",
    "                \n",
    "                seed += 1 \n",
    "            print('\\n')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Iteration:1535--->Loss:24.911477\n",
      "\n",
      "Ashopeus\n",
      "Qotyisaurus\n",
      "Ambjeussaurcanocingasaurus\n",
      "Aurisasaurus\n",
      "Mchosaurusaurus\n",
      "\n",
      "\n",
      "Epoch:10 Iteration:1535--->Loss:21.688634\n",
      "\n",
      "Onwhonsaurus\n",
      "Ans\n",
      "Ambhosaurus\n",
      "Otilunosaurus\n",
      "Mechus\n",
      "\n",
      "\n",
      "Epoch:20 Iteration:1535--->Loss:23.735607\n",
      "\n",
      "Ashnpcoosaurus\n",
      "Antuitnosuyrasous\n",
      "Adbcpyosaurus\n",
      "Allithos\n",
      "Depkus\n",
      "\n",
      "\n",
      "Epoch:30 Iteration:1535--->Loss:27.330989\n",
      "\n",
      "Gydchortonoscunoclogagphiopamatlus\n",
      "Rys\n",
      "Rebpnuatoeshunoblosaurus\n",
      "Aurisncaulupactyahatrpsaurusaurus\n",
      "Fepbuagongwanoblosaurus\n",
      "\n",
      "\n",
      "Epoch:40 Iteration:1535--->Loss:28.927819\n",
      "\n",
      "Auhnphus\n",
      "Auruiropalineapsaurus\n",
      "Aochosaurus\n",
      "Aurisaurus\n",
      "Mophus\n",
      "\n",
      "\n",
      "Epoch:50 Iteration:1535--->Loss:26.488801\n",
      "\n",
      "Ersorous\n",
      "Ossurunngulakaegusnasanvonnirnzngzyronbongongtosau\n",
      "Ossurugunashunabingsaurus\n",
      "Ersurungulakaegusznurhlynzuuhonsaurusyngtongtosaur\n",
      "Mesourus\n",
      "\n",
      "\n",
      "Epoch:60 Iteration:1535--->Loss:25.940408\n",
      "\n",
      "Oraurus\n",
      "Ors\n",
      "Ossurus\n",
      "Ors\n",
      "Mesglongospanahingannolopshyalgusaurus\n",
      "\n",
      "\n",
      "Epoch:70 Iteration:1535--->Loss:27.007590\n",
      "\n",
      "Oreopotongospunopungannniaerhysiayszusaurus\n",
      "Grslisaurus\n",
      "Gesrptorus\n",
      "Orlurunguracopasaurushusaurus\n",
      "Mesrus\n",
      "\n",
      "\n",
      "Epoch:80 Iteration:1535--->Loss:28.788877\n",
      "\n",
      "Nyssuosaurus\n",
      "Salrisaurus\n",
      "Seos\n",
      "N\n",
      "Mess\n",
      "\n",
      "\n",
      "Epoch:90 Iteration:1535--->Loss:27.051371\n",
      "\n",
      "Hepophus\n",
      "Heltiuenglizeops\n",
      "Heaurus\n",
      "Hel\n",
      "Mengus\n",
      "\n",
      "\n",
      "Epoch:100 Iteration:1535--->Loss:28.485915\n",
      "\n",
      "Hros\n",
      "Er\n",
      "Ergps\n",
      "Hraurus\n",
      "Fopeuzonoshsaurus\n",
      "\n",
      "\n",
      "Epoch:110 Iteration:1535--->Loss:27.484226\n",
      "\n",
      "Hen\n",
      "Hel\n",
      "Hen\n",
      "Hil\n",
      "Hen\n",
      "\n",
      "\n",
      "Epoch:120 Iteration:1535--->Loss:24.573471\n",
      "\n",
      "Hopops\n",
      "Hol\n",
      "Hel\n",
      "Hyl\n",
      "Hel\n",
      "\n",
      "\n",
      "Epoch:130 Iteration:1535--->Loss:27.645014\n",
      "\n",
      "Hop\n",
      "Er\n",
      "Besthuguonthynaplorannoltegdan\n",
      "Hal\n",
      "Mep\n",
      "\n",
      "\n",
      "Epoch:140 Iteration:1535--->Loss:25.310584\n",
      "\n",
      "Hophheusaurus\n",
      "Hel\n",
      "Hee\n",
      "Hal\n",
      "Xet\n",
      "\n",
      "\n",
      "Epoch:150 Iteration:1535--->Loss:23.679569\n",
      "\n",
      "Hoptonyngonyhunocin\n",
      "Ceslisaurus\n",
      "Jeprosaurus\n",
      "Hol\n",
      "Deps\n",
      "\n",
      "\n",
      "Epoch:160 Iteration:1535--->Loss:23.262789\n",
      "\n",
      "Enoscorosaurus\n",
      "Nop\n",
      "Nepscurus\n",
      "En\n",
      "Dephyngonghypaploangholopamagiarghus\n",
      "\n",
      "\n",
      "Epoch:170 Iteration:1535--->Loss:23.745596\n",
      "\n",
      "Nop\n",
      "Nop\n",
      "Nepsonasaurus\n",
      "Nos\n",
      "Deps\n",
      "\n",
      "\n",
      "Epoch:180 Iteration:1535--->Loss:26.426858\n",
      "\n",
      "Nops\n",
      "Nops\n",
      "Nepnhuasaurus\n",
      "Nos\n",
      "Dops\n",
      "\n",
      "\n",
      "Epoch:190 Iteration:1535--->Loss:22.735645\n",
      "\n",
      "Nodthosaurus\n",
      "Nos\n",
      "Nes\n",
      "Nli\n",
      "Deps\n",
      "\n",
      "\n",
      "Epoch:200 Iteration:1535--->Loss:25.189837\n",
      "\n",
      "Nop\n",
      "Nop\n",
      "Nepthuguongdunapuosaurus\n",
      "Nop\n",
      "Dops\n",
      "\n",
      "\n",
      "Epoch:210 Iteration:1535--->Loss:22.735218\n",
      "\n",
      "N\n",
      "Nop\n",
      "Nops\n",
      "N\n",
      "Dops\n",
      "\n",
      "\n",
      "Epoch:220 Iteration:1535--->Loss:25.110984\n",
      "\n",
      "Nopowosaurus\n",
      "Nos\n",
      "Nes\n",
      "N\n",
      "Dops\n",
      "\n",
      "\n",
      "Epoch:230 Iteration:1535--->Loss:24.511891\n",
      "\n",
      "Nops\n",
      "Nos\n",
      "Nes\n",
      "N\n",
      "Dop\n",
      "\n",
      "\n",
      "Epoch:240 Iteration:1535--->Loss:28.309966\n",
      "\n",
      "En\n",
      "N\n",
      "Nopodanus\n",
      "En\n",
      "De\n",
      "\n",
      "\n",
      "Epoch:250 Iteration:1535--->Loss:25.164694\n",
      "\n",
      "N\n",
      "N\n",
      "Nops\n",
      "N\n",
      "Dops\n",
      "\n",
      "\n",
      "Epoch:260 Iteration:1535--->Loss:27.685277\n",
      "\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "Des\n",
      "\n",
      "\n",
      "Epoch:270 Iteration:1535--->Loss:24.972123\n",
      "\n",
      "En\n",
      "N\n",
      "N\n",
      "En\n",
      "Dep\n",
      "\n",
      "\n",
      "Epoch:280 Iteration:1535--->Loss:24.790702\n",
      "\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "Dep\n",
      "\n",
      "\n",
      "Epoch:290 Iteration:1535--->Loss:24.196910\n",
      "\n",
      "H\n",
      "E\n",
      "Epsanratonthunaeinsaurus\n",
      "H\n",
      "Dnpyonionghynzhingosaurus\n",
      "\n",
      "\n",
      "Epoch:300 Iteration:1535--->Loss:24.367742\n",
      "\n",
      "Epsps\n",
      "N\n",
      "N\n",
      "Ery\n",
      "Dop\n",
      "\n",
      "\n",
      "Epoch:310 Iteration:1535--->Loss:25.162856\n",
      "\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "Depyonosaurus\n",
      "\n",
      "\n",
      "Epoch:320 Iteration:1535--->Loss:24.349201\n",
      "\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "D\n",
      "\n",
      "\n",
      "Epoch:330 Iteration:1535--->Loss:23.972049\n",
      "\n",
      "Bapkopyoonososaurus\n",
      "N\n",
      "N\n",
      "Bis\n",
      "Dep\n",
      "\n",
      "\n",
      "Epoch:340 Iteration:1535--->Loss:25.693307\n",
      "\n",
      "D\n",
      "D\n",
      "Depepangongx\n",
      "D\n",
      "D\n",
      "\n",
      "\n",
      "Epoch:350 Iteration:1535--->Loss:24.740039\n",
      "\n",
      "D\n",
      "D\n",
      "D\n",
      "D\n",
      "D\n",
      "\n",
      "\n",
      "Epoch:360 Iteration:1535--->Loss:25.802923\n",
      "\n",
      "D\n",
      "D\n",
      "D\n",
      "D\n",
      "D\n",
      "\n",
      "\n",
      "Epoch:370 Iteration:1535--->Loss:26.872205\n",
      "\n",
      "D\n",
      "D\n",
      "Depstraenospunnotongabolopamasugusaurus\n",
      "D\n",
      "Dops\n",
      "\n",
      "\n",
      "Epoch:380 Iteration:1535--->Loss:25.168947\n",
      "\n",
      "D\n",
      "Wlliisaurus\n",
      "Wchnosaurus\n",
      "D\n",
      "Dops\n",
      "\n",
      "\n",
      "Epoch:390 Iteration:1535--->Loss:24.510140\n",
      "\n",
      "D\n",
      "W\n",
      "D\n",
      "D\n",
      "D\n",
      "\n",
      "\n",
      "Epoch:400 Iteration:1535--->Loss:24.707889\n",
      "\n",
      "Dophonynsaurus\n",
      "Jgyuizchus\n",
      "X\n",
      "Donisaurus\n",
      "Dops\n",
      "\n",
      "\n",
      "Epoch:410 Iteration:1535--->Loss:24.804026\n",
      "\n",
      "Dophengosaurus\n",
      "Wogivunnitusaurus\n",
      "Wchengosaurus\n",
      "Don\n",
      "Dops\n",
      "\n",
      "\n",
      "Epoch:420 Iteration:1535--->Loss:23.560233\n",
      "\n",
      "Donophus\n",
      "Wytiisaurus\n",
      "Wchnosaurus\n",
      "Don\n",
      "Donlus\n",
      "\n",
      "\n",
      "Epoch:430 Iteration:1535--->Loss:24.820301\n",
      "\n",
      "Dophenangonghynoplogannousaurus\n",
      "Dosaurus\n",
      "Dononyzus\n",
      "Dor\n",
      "Donasaurus\n",
      "\n",
      "\n",
      "Epoch:440 Iteration:1535--->Loss:26.657233\n",
      "\n",
      "Donoceratops\n",
      "Dontisaurus\n",
      "Donoprosaurus\n",
      "Donithosaurus\n",
      "Donosaurus\n",
      "\n",
      "\n",
      "Epoch:450 Iteration:1535--->Loss:23.154616\n",
      "\n",
      "Dophonynynoseunoplongunninospon\n",
      "Don\n",
      "Dnx\n",
      "Dgnisaurus\n",
      "Donosaurus\n",
      "\n",
      "\n",
      "Epoch:460 Iteration:1535--->Loss:22.726041\n",
      "\n",
      "Donnan\n",
      "Don\n",
      "Donnatorsaurus\n",
      "Donisaurus\n",
      "Donosaurus\n",
      "\n",
      "\n",
      "Epoch:470 Iteration:1535--->Loss:24.092823\n",
      "\n",
      "Donnantanonthanohinosaurus\n",
      "Don\n",
      "Donnas\n",
      "Donisaurus\n",
      "Donosaurus\n",
      "\n",
      "\n",
      "Epoch:480 Iteration:1535--->Loss:25.369071\n",
      "\n",
      "Donophysaurus\n",
      "Donyisaurus\n",
      "Dnxensaurus\n",
      "Dgyigopautopaps\n",
      "Donosaurus\n",
      "\n",
      "\n",
      "Epoch:490 Iteration:1535--->Loss:26.424063\n",
      "\n",
      "Dophon\n",
      "Don\n",
      "Dennosaurus\n",
      "Danisaurus\n",
      "Donosaurus\n",
      "\n",
      "\n",
      "Epoch:500 Iteration:1535--->Loss:24.344368\n",
      "\n",
      "Dgnnnosaurus\n",
      "Don\n",
      "Dhnnosaurus\n",
      "Dusigeegiusaurus\n",
      "Donnus\n",
      "\n",
      "\n",
      "Epoch:510 Iteration:1535--->Loss:24.210409\n",
      "\n",
      "Wawhongasaurus\n",
      "Wasuinobyuyopaptonnus\n",
      "Wknopronasthunocungogoniongdasugusaurus\n",
      "Wuu\n",
      "Dops\n",
      "\n",
      "\n",
      "Epoch:520 Iteration:1535--->Loss:24.825907\n",
      "\n",
      "Wthnopoguangxunokingggeniongdaniasaurus\n",
      "Wasuinobus\n",
      "Whnopoguangxunokingggeniongdaniasaurus\n",
      "Wlivunnitusaurus\n",
      "Dops\n",
      "\n",
      "\n",
      "Epoch:530 Iteration:1535--->Loss:26.201974\n",
      "\n",
      "Wawhoeus\n",
      "Wortisaurus\n",
      "Whnnptor\n",
      "Wurus\n",
      "Donosaurus\n",
      "\n",
      "\n",
      "Epoch:540 Iteration:1535--->Loss:25.048435\n",
      "\n",
      "Wakenosaurus\n",
      "Wagilunnitonosaurus\n",
      "Whnnus\n",
      "Wurispenrinnus\n",
      "Donaongonghanobinosaurus\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
